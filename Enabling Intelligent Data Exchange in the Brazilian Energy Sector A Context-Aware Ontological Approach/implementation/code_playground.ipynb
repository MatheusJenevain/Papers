{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from owlready2 import get_ontology\n",
    "from owlready2 import sync_reasoner_hermit\n",
    "import traceback\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining file path\n",
    "directory = 'C:\\\\Users\\\\Matheus\\\\Documents\\\\GitHub\\\\Papers\\\\Enabling Intelligent Data Exchange in the Brazilian Energy Sector A Context-Aware Ontological Approach\\\\ontology\\\\imports'\n",
    "filename = 'oec-extracted.owl'\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "try:\n",
    "    # Loading the ontology\n",
    "    onto = get_ontology(\"file://\" + file_path).load()\n",
    "    print(\"Ontology loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # if load fails\n",
    "    print(\"Error occurred during ontology loading:\")\n",
    "    print(e)\n",
    "\n",
    "#check for inconsistensies\n",
    "inconsistencies = list(onto.inconsistent_classes())\n",
    "if inconsistencies:\n",
    "    print(\"Ontology is inconsistent!\")\n",
    "    print(\"Inconsistencies:\")\n",
    "    for inconsistency in inconsistencies:\n",
    "        print(f\"Inconsistency found involving: {inconsistency}\")\n",
    "else:\n",
    "    print(\"Ontology is consistent.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first sync to check if reasoner works\n",
    "try:\n",
    "    with onto:\n",
    "        sync_reasoner_hermit()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking inferred objects and its connections\n",
    "inferred_properties = set()\n",
    "\n",
    "for prop in onto.object_properties():\n",
    "    try:\n",
    "        for subj, obj in prop.get_relations():\n",
    "            inferred_properties.add((prop, subj, obj))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing object property '{prop.name}': {e}\")\n",
    "\n",
    "# Print inferred object properties and their usage\n",
    "for prop, subj, obj in inferred_properties:\n",
    "    try:\n",
    "        subj_type = \"Class\" if subj in onto.classes() else \"Individual\" if subj in onto.individuals() else \"Unknown\"\n",
    "        obj_type = \"Class\" if obj in onto.classes() else \"Individual\" if obj in onto.individuals() else \"Unknown\"\n",
    "\n",
    "        print(f\"Object Property '{prop.name}' inferred on {obj_type.lower()} '{obj.name}' because of '{subj.name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while formatting output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sparql queries\n",
    "query = \"\"\"\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX oec: <http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted#>\n",
    "\n",
    "    SELECT ?individual1 ?individual2\n",
    "    WHERE {\n",
    "        ?individual1 oec:termHasMeaningByContext ?individual2 .\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Executing SPARQL query\n",
    "    results = onto.world.sparql(query)\n",
    "\n",
    "    # Printing results\n",
    "    for row in results:\n",
    "        individual1 = row[0]\n",
    "        individual2 = row[1]\n",
    "        print(f\"Individual connected through 'termHasMeaningByContext': {individual1.name} to {individual2.name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while executing the SPARQL query:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX oec: <http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted#>\n",
    "\n",
    "    SELECT ?individual1 ?property ?individual2\n",
    "    WHERE {\n",
    "        ?individual1 ?property ?individual2 .\n",
    "        FILTER (isIRI(?individual1) && isIRI(?individual2) && ?property != rdf:type)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    results = onto.world.sparql(query)\n",
    "\n",
    "    property_names = {\n",
    "        \"http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted#actorHasOrganization\": \"Actor Has Organization\",\n",
    "    }\n",
    "\n",
    "    for row in results:\n",
    "        individual1 = row[0]\n",
    "        property_uri = row[1]\n",
    "        individual2 = row[2]\n",
    "        \n",
    "        property_name = property_names.get(str(property_uri), str(property_uri))\n",
    "        \n",
    "        print(f\"Individual {individual1.name} is connected to {individual2.name} through property {property_name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Error occurred while executing the SPARQL query:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing inferred information for each class, property, and individual\n",
    "print(\"Inferred Classes:\")\n",
    "for cls in onto.classes():\n",
    "    cls_name = cls.name.split('#')[-1] if '#' in cls.name else cls.name\n",
    "    print(f\"\\nClass: {cls_name}\")\n",
    "    instances = list(cls.instances())\n",
    "    if instances:\n",
    "        instance_names = [inst.name.split('#')[-1] if '#' in inst.name else inst.name for inst in instances]\n",
    "        print(f\"Inferred Instances: {instance_names}\")\n",
    "    else:\n",
    "        print(\"No inferred instances.\")\n",
    "\n",
    "print(\"\\nInferred Object Properties:\")\n",
    "for prop in onto.object_properties():\n",
    "    prop_name = prop.name.split('#')[-1] if '#' in prop.name else prop.name\n",
    "    print(f\"\\nObject Property: {prop_name}\")\n",
    "    relations = [(subj.name.split('#')[-1] if '#' in subj.name else subj.name,\n",
    "                  obj.name.split('#')[-1] if '#' in obj.name else obj.name) for subj, obj in prop.get_relations()]\n",
    "    if relations:\n",
    "        for subj, obj in relations:\n",
    "            print(f\"Subject: {subj}, Object: {obj}\")\n",
    "    else:\n",
    "        print(\"No inferred relations.\")\n",
    "\n",
    "print(\"\\nInferred Data Properties:\")\n",
    "for data_prop in onto.data_properties():\n",
    "    data_prop_name = data_prop.name.split('#')[-1] if '#' in data_prop.name else data_prop.name\n",
    "    print(f\"\\nData Property: {data_prop_name}\")\n",
    "    values = [(subj.name.split('#')[-1] if '#' in subj.name else subj.name, value) for subj, value in data_prop.get_relations()]\n",
    "    if values:\n",
    "        for subj, value in values:\n",
    "            print(f\"Subject: {subj}, Value: {value}\")\n",
    "    else:\n",
    "        print(\"No inferred values.\")\n",
    "\n",
    "print(\"\\nInferred Individuals:\")\n",
    "for ind in onto.individuals():\n",
    "    ind_name = ind.name.split('#')[-1] if '#' in ind.name else ind.name\n",
    "    print(f\"\\nIndividual: {ind_name}\")\n",
    "    types = [typ.name.split('#')[-1] if '#' in typ.name else typ.name for typ in ind.is_a]\n",
    "    if types:\n",
    "        print(f\"Inferred Types: {types}\")\n",
    "    else:\n",
    "        print(\"No inferred types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ontology using its IRI\n",
    "onto = get_ontology(\"http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted\").load()\n",
    "\n",
    "# Retrieve individuals starting with \"ExchangedTerm\"\n",
    "matching_individuals = [ind for ind in onto.individuals() if ind.name.startswith(\"ExchangedTerm\")]\n",
    "\n",
    "# Define the object property to focus on\n",
    "target_property = onto.termHasMeaningByContext\n",
    "\n",
    "# Iterate through the filtered individuals to find those with the target object property\n",
    "for term_individual in matching_individuals:\n",
    "    # Check if the individual has the target object property\n",
    "    if hasattr(term_individual, \"termHasMeaningByContext\"):\n",
    "        related_property = term_individual.termHasMeaningByContext\n",
    "        if related_property:\n",
    "            print(f\"Individual '{term_individual.name}' has the 'termHasMeaningByContext' object property.\")\n",
    "            for related_ind in related_property:\n",
    "                print(f\" - Related individual: '{related_ind.name}'\")\n",
    "        else:\n",
    "            print(f\"Individual '{term_individual.name}' has 'termHasMeaningByContext' but no related individuals.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating termContentString based on context. If none is available, it will use WordNet definitions\n",
    "from owlready2 import *\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def extract_context_meaning(term_individual, nltk_context):\n",
    "    context_meaning = \"\"\n",
    "    connected_to = []\n",
    "\n",
    "    try:\n",
    "        if hasattr(term_individual, \"termHasMeaningByContext\"):\n",
    "            related_property = term_individual.termHasMeaningByContext\n",
    "            for related_ind in related_property:\n",
    "                related_ind_name = related_ind.name if related_ind.name else \"\"\n",
    "                connected_to.append(related_ind_name)\n",
    "\n",
    "        if hasattr(term_individual, \"termLexicon\") and term_individual.termLexicon:\n",
    "            term_lexicon = term_individual.termLexicon[0]  # Assuming a single value\n",
    "            if term_lexicon in nltk_context.get(\"ContextDERHousehold\", {}):\n",
    "                context_meaning = nltk_context[\"ContextDERHousehold\"][term_lexicon]\n",
    "                source = \"NLTK Context\"\n",
    "            else:\n",
    "                synsets = wordnet.synsets(term_lexicon)\n",
    "                if synsets:\n",
    "                    context_meaning = synsets[0].definition()\n",
    "                    source = \"WordNet\"\n",
    "\n",
    "            if context_meaning:\n",
    "                if not hasattr(term_individual, \"termContentString\"):\n",
    "                    term_individual.termContentString = []\n",
    "                term_individual.termContentString.append(context_meaning)\n",
    "                return context_meaning, connected_to, term_lexicon, source\n",
    "            else:\n",
    "                return None, connected_to, term_lexicon, source\n",
    "        else:\n",
    "            return None, connected_to, None, None\n",
    "    except AttributeError:\n",
    "        return None, [], None, None\n",
    "\n",
    "def process_individuals(onto, file_path):\n",
    "    matching_individuals = [ind for ind in onto.individuals() if ind.name.startswith(\"ExchangedTerm\")]\n",
    "\n",
    "    for term_individual in matching_individuals:\n",
    "        context_meaning, connected_to, term_lexicon, source = extract_context_meaning(term_individual, nltk_context)\n",
    "\n",
    "        if context_meaning:\n",
    "            with onto:\n",
    "                sync_reasoner()  # Saving changes to the ontology\n",
    "                onto.save(file_path)\n",
    "                \n",
    "            print(f\"Individual: '{term_individual.name}'\")\n",
    "            print(f\" - Object Property: 'termHasMeaningByContext'\")\n",
    "            print(f\" - Connects to: {', '.join(connected_to) if connected_to else 'None'}\")\n",
    "            if term_lexicon:\n",
    "                print(f\" - Term Lexicon: {term_lexicon}\")\n",
    "            print(f\" - Context Meaning: {context_meaning} (Source: {source})\")\n",
    "        else:\n",
    "            print(\" - No matching context found. NLTK WordNet used.\")\n",
    "\n",
    "def main():\n",
    "    directory = 'C:\\\\Users\\\\Matheus\\\\Documents\\\\GitHub\\\\Papers\\\\Enabling Intelligent Data Exchange in the Brazilian Energy Sector A Context-Aware Ontological Approach\\\\ontology\\\\imports'\n",
    "    filename = 'oec-extracted.owl'\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    try:\n",
    "        onto = get_ontology(\"file://\" + file_path).load()\n",
    "        print(\"Ontology loaded successfully!\")\n",
    "        process_individuals(onto, file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during ontology loading:\")\n",
    "        print(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nltk_context = {\n",
    "        \"ContextDERHousehold\": {\n",
    "            \"Storage\": \"cells\",\n",
    "        }\n",
    "    }\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Function to get meanings of words using WordNet\n",
    "def get_meanings(word):\n",
    "    meanings = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            meanings.append(lemma.name())\n",
    "    return list(set(meanings))\n",
    "\n",
    "# Sample text\n",
    "text = \"NLTK is a powerful library for natural language processing.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Dictionary to store words and their meanings\n",
    "words_meanings = {}\n",
    "\n",
    "# Get meanings for each word\n",
    "for word in words:\n",
    "    meanings = get_meanings(word)\n",
    "    words_meanings[word] = meanings\n",
    "\n",
    "# Print words and their meanings\n",
    "for word, meanings in words_meanings.items():\n",
    "    print(f\"Word: {word}\")\n",
    "    if meanings:\n",
    "        print(f\"Meanings: {', '.join(meanings)}\")\n",
    "    else:\n",
    "        print(\"No meanings found\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
